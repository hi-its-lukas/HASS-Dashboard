Weiter zum Inhalt
Chatverlauf





Teammitglieder einladen
Du:
MSE Livestream Debug - Modal Black Screen Issue
Problem Summary
UniFi Protect camera livestreaming works for thumbnail views (first client to connect) but shows black screen in modal (second client joining existing session).

Architecture
Server-Side (Node.js)
ws-proxy.ts: WebSocket server on port 6000, proxies livestream data
protect-livestream.ts: Uses unifi-protect library for native fMP4 streaming
Sessions are shared between multiple clients viewing the same camera
Client-Side (React/Next.js)
WebRTCPlayer.tsx: MSE (Media Source Extensions) player
Uses MediaSource API with SourceBuffer for fMP4 playback
Data Flow
First Client (Thumbnail) - WORKS
Client connects WebSocket to /ws/livestream/{cameraId}
Server starts new ProtectLivestream for camera
Server extracts codec from init segment, sends to client
Server sends init segment (ftyp+moov) to client
Server sends moof+mdat chunks continuously
Client creates SourceBuffer, appends data, video plays
Second Client (Modal) - FAILS
Client connects WebSocket to /ws/livestream/{cameraId}
Server finds existing session, reuses it
Server sends cached codec immediately
Server waits 100ms (for client to create SourceBuffer)
Server sends cached init segment
Server adds client to receive moof chunks
Client video stays black
Current Code
protect-livestream.ts (Session Reuse)
const existingSession = this.sessions.get(cameraId)
if (existingSession) {
  // DON'T add to clients yet - wait until init segment is sent
  if (onCodec) {
    existingSession.codecCallbacks.add(onCodec)
    if (existingSession.lastCodec) {
      onCodec(existingSession.lastCodec) // Send codec immediately
    }
  }
  
  if (existingSession.initSegment && existingSession.foundInit) {
    const initSegmentCopy = Buffer.from(existingSession.initSegment)
    setTimeout(() => {
      onData(initSegmentCopy) // Send init segment
      existingSession.initSegmentSent.add(onData)
      existingSession.clients.add(onData) // THEN add to receive moof
    }, 100)
  }
  return true
}
WebRTCPlayer.tsx (Client)
// When codec message received
if (msg.type === 'codec') {
  pendingCodec.current = msg.codec
  tryInitializeSourceBuffer()
}
// MediaSource sourceopen handler
mediaSource.addEventListener('sourceopen', () => {
  sourceOpenReady.current = true
  tryInitializeSourceBuffer()
})
// Initialize SourceBuffer
const tryInitializeSourceBuffer = () => {
  if (!ms || !codec || !sourceOpenReady.current) return false
  if (ms.readyState !== 'open') return false
  if (sourceBufferRef.current) return true // Already exists
  
  const sb = ms.addSourceBuffer(video/mp4; codecs="${codec}")
  // ... setup event handlers
  
  // Flush queued chunks
  if (bufferQueue.current.length > 0) {
    processQueue()
  }
}
// Binary data handler
if (event.data instanceof ArrayBuffer) {
  appendBuffer(event.data) // Queues data if SourceBuffer not ready
}
What We've Tried
Sending init segment immediately - fails (SourceBuffer not ready)
100ms delay before sending init segment - still fails
Sending codec before init segment - still fails
Adding client to moof list AFTER init segment - still fails
Debug Observations
Thumbnail (works)
WebSocket connects
MediaSource opens
Codec received, SourceBuffer created
Init segment received, appended
moof chunks received, appended
Video plays with buffered ranges: 0.00-X.XXs
Modal (fails)
WebSocket connects
MediaSource opens
Codec received, SourceBuffer created
Init segment received, appended
moof chunks received, appended
Video stays black, buffered ranges: none or very small
Questions
Why does the same init segment work for thumbnail but not modal?
Is there a timing issue with MSE when reusing the same codec/init?
Could the cached init segment be corrupted or incompatible?
Is there something special about joining a live fMP4 stream mid-session?
fMP4 Structure
ftyp: File type box (part of init)
moov: Movie box with track info (part of init)
moof: Movie fragment header (each fragment)
mdat: Media data (each fragment)
The init segment (ftyp+moov) must be sent before any moof+mdat.

Environment
Next.js 14 with TypeScript
unifi-protect library for camera streaming
Safari (iPhone), Edge (Mac), Fully Kiosk (Android)
Cloudflare Tunnel for HTTPS
Key Insight Needed
Why does appending the same init segment + moof chunks work for the first client but produce a black screen for the second client joining the same stream?

Full Code Files
WebRTCPlayer.tsx (Client-Side MSE Player)
'use client'
import { useEffect, useRef, useState, useCallback } from 'react'
import { Loader2, AlertCircle } from 'lucide-react'
interface WebRTCPlayerProps {
  cameraId: string
  className?: string
  autoPlay?: boolean
  onError?: (error: string) => void
}
export default function WebRTCPlayer({ 
  cameraId, 
  className = '',
  autoPlay = true,
  onError 
}: WebRTCPlayerProps) {
  const videoRef = useRef<HTMLVideoElement>(null)
  const wsRef = useRef<WebSocket | null>(null)
  const mediaSourceRef = useRef<MediaSource | null>(null)
  const sourceBufferRef = useRef<SourceBuffer | null>(null)
  const bufferQueue = useRef<ArrayBuffer[]>([])
  const pendingCodec = useRef<string | null>(null)
  const sourceOpenReady = useRef(false)
  const isConnectedRef = useRef(false)
  const [status, setStatus] = useState<'idle' | 'connecting' | 'connected' | 'error'>('idle')
  const [error, setError] = useState<string | null>(null)
  const cleanup = useCallback(() => {
    isConnectedRef.current = false
    if (wsRef.current) {
      wsRef.current.close()
      wsRef.current = null
    }
    if (mediaSourceRef.current && mediaSourceRef.current.readyState === 'open') {
      try {
        mediaSourceRef.current.endOfStream()
      } catch (e) {}
    }
    mediaSourceRef.current = null
    sourceBufferRef.current = null
    bufferQueue.current = []
    pendingCodec.current = null
    sourceOpenReady.current = false
    appendedCount.current = 0
  }, [])
  const appendedCount = useRef(0)
  
  const processQueue = useCallback(() => {
    const sb = sourceBufferRef.current
    if (!sb || sb.updating || bufferQueue.current.length === 0) return
    
    const chunk = bufferQueue.current.shift()
    if (chunk) {
      try {
        sb.appendBuffer(chunk)
        appendedCount.current++
      } catch (e) {
        console.error('[LivestreamPlayer] appendBuffer error:', e)
      }
    }
  }, [])
  const appendBuffer = useCallback((data: ArrayBuffer) => {
    bufferQueue.current.push(data)
    const sb = sourceBufferRef.current
    if (sb && !sb.updating) {
      processQueue()
    }
  }, [processQueue])
  const tryInitializeSourceBuffer = useCallback(() => {
    const ms = mediaSourceRef.current
    const codec = pendingCodec.current
    
    if (!ms || !codec || !sourceOpenReady.current) return false
    if (ms.readyState !== 'open') return false
    if (sourceBufferRef.current) return true
    
    let mimeCodec = video/mp4; codecs="${codec}"
    
    if (!MediaSource.isTypeSupported(mimeCodec)) {
      const videoCodec = codec.split(',')[0].trim()
      mimeCodec = video/mp4; codecs="${videoCodec}"
    }
    
    try {
      const sb = ms.addSourceBuffer(mimeCodec)
      sourceBufferRef.current = sb
      sb.mode = 'segments'
      
      sb.addEventListener('updateend', () => {
        processQueue()
        
        if (videoRef.current && sb.buffered.length > 0 && videoRef.current.paused) {
          const bufferedEnd = sb.buffered.end(0)
          if (bufferedEnd > 0.1) {
            videoRef.current.play().catch(e => {})
          }
        }
      })
      
      isConnectedRef.current = true
      setStatus('connected')
      
      if (bufferQueue.current.length > 0) {
        processQueue()
      }
      
      return true
    } catch (e) {
      setError('Failed to initialize video player')
      setStatus('error')
      return false
    }
  }, [processQueue])
  startStreamingRef.current = () => {
    if (!videoRef.current) return
    cleanup()
    setStatus('connecting')
    const protocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:'
    const wsUrl = ${protocol}//${window.location.host}/ws/livestream/${encodeURIComponent(cameraId)}
    
    const ws = new WebSocket(wsUrl)
    wsRef.current = ws
    ws.binaryType = 'arraybuffer'
    
    const mediaSource = new MediaSource()
    mediaSourceRef.current = mediaSource
    videoRef.current.src = URL.createObjectURL(mediaSource)
    
    mediaSource.addEventListener('sourceopen', () => {
      sourceOpenReady.current = true
      tryInitializeSourceBuffer()
    })
    
    ws.onmessage = (event) => {
      if (typeof event.data === 'string') {
        const msg = JSON.parse(event.data)
        if (msg.type === 'codec') {
          pendingCodec.current = msg.codec
          tryInitializeSourceBuffer()
        }
      } else if (event.data instanceof ArrayBuffer) {
        appendBuffer(event.data)
      }
    }
  }
  useEffect(() => {
    if (autoPlay) startStreaming()
    return cleanup
  }, [autoPlay, cameraId, cleanup, startStreaming])
  return (
    <div className={relative bg-black ${className}}>
      <video ref={videoRef} autoPlay playsInline muted className="w-full h-full object-cover" />
    </div>
  )
}
protect-livestream.ts (Server-Side Session Management - Key Section)
async startStream(
  cameraId: string, 
  onData: (data: Buffer) => void,
  onCodec?: (codec: string) => void
): Promise<boolean> {
  const existingSession = this.sessions.get(cameraId)
  if (existingSession) {
    console.log('[ProtectLivestream] Reusing existing stream - adding client')
    
    // DON'T add to clients yet - wait until init segment is sent
    if (onCodec) {
      existingSession.codecCallbacks.add(onCodec)
      if (existingSession.lastCodec) {
        console.log('[ProtectLivestream] Sending cached codec:', existingSession.lastCodec)
        onCodec(existingSession.lastCodec)
      }
    }
    
    // Send cached init segment with delay, then add to clients for moof data
    if (existingSession.initSegment && existingSession.foundInit) {
      const initSegmentCopy = Buffer.from(existingSession.initSegment)
      setTimeout(() => {
        try {
          // First send init segment
          onData(initSegmentCopy)
          existingSession.initSegmentSent.add(onData)
          // THEN add to clients to receive moof data
          existingSession.clients.add(onData)
        } catch (e) {
          console.error('[ProtectLivestream] Error sending init segment:', e)
        }
      }, 100) // 100ms delay for client to initialize SourceBuffer
    } else {
      existingSession.clients.add(onData)
    }
    
    return true
  }
  
  // ... start new stream for first client
}
ws-proxy.ts (WebSocket Proxy - Key Section)
const onCodec = (codec: string) => {
  if (clientWs.readyState === WebSocket.OPEN && !codecSent) {
    clientWs.send(JSON.stringify({ type: 'codec', codec }))
    codecSent = true
  }
}
const onData = (data: Buffer) => {
  if (clientWs.readyState === WebSocket.OPEN) {
    clientWs.send(data)
  }
}
const started = await manager.startStream(cameraId, onData, onCodec)
Specific Question for ChatGPT/Gemini
The modal (second client) receives:

Codec message (immediately)
Init segment ftyp+moov (after 100ms delay)
moof+mdat chunks (after init segment sent)
But the video stays black. The thumbnail (first client) works perfectly with the same data flow.

What could cause the second client to fail even though:

The SourceBuffer is created with the same codec
The same init segment (ftyp+moov) is appended
moof chunks are appended after the init segment
Hypothesis: Keyframe Problem
In fMP4/fragmented MP4:

Video is encoded with I-frames (keyframes) and P/B-frames (dependent frames)
P/B-frames depend on previous frames to decode
First client gets stream from start: init → keyframe → P-frames → keyframe → ...
Second client joins mid-stream: init → P-frames (no keyframe!) → ...
If the second client starts receiving data between keyframes, the decoder cannot render any frames until the next keyframe arrives.

Questions:
How can we detect keyframes in moof chunks to start new clients at a keyframe boundary?
Should we cache the last keyframe moof+mdat and replay it to new clients?
Is there a way to tell MSE/SourceBuffer to wait for a keyframe?
Does unifi-protect library provide keyframe detection or segment boundaries?
ChatGPT:
Thanks for the detailed breakdown. To help you best, could you please clarify:

Are you using any codec-specific options or parameters in the camera setup (e.g., H.264 profile, GOP length)?

Do you know the keyframe interval for your Unifi Protect stream?

Would you like help implementing keyframe detection on the server side (e.g., parsing moof boxes) or evaluating if unifi-protect exposes keyframe metadata?

Is the goal to make the modal join smoothly mid-stream without rebuffering, or is a short startup delay acceptable if it ensures playback starts at a keyframe?

This will help determine whether you need client- or server-side solutions, or both.

Du:
1. Codec/Profile:

UniFi Protect verwendet H.264 High Profile (avc1.640020 oder avc1.4d401f je nach Auflösung)
Keine speziellen Codec-Optionen auf Server-Seite konfiguriert - der Stream kommt direkt von der Kamera
2. Keyframe Interval:

UniFi Protect Kameras verwenden typischerweise 1 Sekunde GOP (Keyframe alle ~1s)
Konfigurierbar in der UniFi Protect UI pro Kamera
Die unifi-protect Library bietet ProtectCameraConfig.channels[].idrInterval zur Abfrage
3. Keyframe Detection:
Ja, wir können Keyframes server-seitig erkennen durch:

Parsen von moof → traf → trun Einträgen
Prüfen der sample_flags: Wenn Bit sample_is_non_sync_sample (0x00010000) nicht gesetzt ist, ist es ein Keyframe
4. Ziel:
Ein kurzer Startup-Delay (bis zum nächsten Keyframe, max ~1 Sekunde) ist akzeptabel - stabiles Playback ist wichtiger als sofortiger Start.
ChatGPT:
Great. I’ll look into best practices for enabling MSE livestream playback from mid-session using fMP4, with a focus on keyframe-aware start behavior. I’ll explore how to detect keyframes in fMP4 moof+mdat fragments, strategies for caching or waiting until a keyframe, and whether MSE requires explicit sync points for playback to begin correctly.

I’ll get back to you with implementation guidance shortly. Feel free to continue chatting in the meantime!

Research completed in 14m· 17 Quellen· 58 Suchen
ChatGPT:
MSE Livestream Black Screen – Root Cause and Solution
Codec and Initialization Segment
UniFi Protect cameras stream H.264 video (commonly High or Main Profile, e.g. avc1.640020 for high-res feeds). The codec and init segment (ftyp+moov) are identical for all clients, and the first client’s successful playback confirms these are correct. In other words, there is no codec mismatch or corruption causing the second client’s black screen. Reusing the same codec string and initialization segment for new viewers is fine – the issue lies elsewhere (likely in timing and video frame dependencies, not in the codec data itself).
Keyframe Interval (GOP Length) in Protect Streams
UniFi Protect cameras typically produce an I-frame (keyframe) about every 1 second by default (configurable per camera). This keyframe interval (often called IDR interval or GOP length) means all intermediate frames rely on the last keyframe. For example, if the camera is 20 FPS with a 1-second GOP, you get one IDR frame followed by ~19 P-frames until the next keyframe. (In some versions, Protect’s default was a 5-second interval, but it’s usually set to 1s for low-latency streaming.) The unifi-protect API even exposes an idrInterval setting for each video channel, indicating how many frames (or seconds) between keyframes. The keyframe frequency is crucial for new clients: it determines the maximum delay before a joining client can see a decodable frame.
Joining Mid-Stream Requires a Keyframe
The core issue is that the second (modal) client is joining the stream in the middle of a GOP, without an initial keyframe. In H.264, video playback must start on a keyframe – if you begin feeding the decoder only interdependent P-frames/B-frames with no preceding IDR frame, the decoder has no reference and cannot render a picture
. This is why the thumbnail (first client) works: it started from the stream’s beginning (which began with an IDR in the init segment), so every frame had the needed reference. The modal client, however, attached to an ongoing session and likely received a “moof” fragment containing only delta frames at first. Without the corresponding IDR, those frames “make no sense and produce no picture on their own”
. As a result, the video element remains black until a keyframe arrives. Additional Timing Factor: When the second client appends its first data chunk, that fragment’s timestamps are likely well past 0 (e.g. 10s into the stream). The media buffer now starts at a later time with nothing buffered at t=0. The video element by default begins at currentTime = 0, outside the buffered range – leading to a stall. The player will not display anything until playback position reaches the buffered range. In practice, the second client’s video tag is stuck at time 0 with no frame to show. Even though the code calls video.play(), the player cannot start rendering because it hasn’t received a keyframe at the current time. This compound effect (missing IDR + timeline gap) explains why the modal view shows no video and why the buffered TimeRange appeared “none or very small.” It’s not that the cached init segment is wrong – it’s that the first media fragment for the second client was not a sync frame, and thus nothing decoded or added to the playable buffer.
Verifying Keyframes in fMP4 Stream
Yes – we can detect keyframe boundaries on the server side. In fragmented MP4 (fMP4), each moof box contains track run entries (trun) with flags indicating frame type. Specifically, there’s a sample_flags field where the sample_is_non_sync_sample bit is set for non-key frames. If that flag is 0 for the first sample in a fragment, it means the fragment starts with a sync sample (keyframe)
. In other words, by parsing each incoming moof from the Protect stream, the server can identify whether it begins with a keyframe. The UniFi Protect library doesn’t explicitly call this out for us, but we can parse the bytes or use a library (e.g. mp4box or similar) to check those flags. Additionally, since we know the camera’s keyframe interval (e.g. ~1 second), we expect roughly every Nth fragment to be an IDR. Many streaming implementations fragment at keyframe boundaries by design (ftyp+moov init, then each moof+mdat contains one GOP or starts with an IDR). We should confirm if Protect’s fMP4 behaves this way. If so, each IDR frame will coincide with a new fragment, making detection easier. Keyframe caching: It would be wise to store the last keyframe fragment in memory. For example, whenever a moof with a sync sample comes through, keep a copy of that moof+mdat. This way, when a new client joins, we have an IDR frame ready to send (even if the next keyframe is nearly a full interval away). We could either send this cached keyframe immediately or use it as a fallback if the next one is delayed.
Solution – Start New Clients at a Keyframe
To fix the black-screen issue, ensure that any new viewer begins receiving data at a keyframe boundary. There are a couple of approaches to achieve this:
Wait for the Next Keyframe: When a second client connects to an existing session, do not immediately pipe through the live P-frames. Instead, send them the codec info (so they can create a SourceBuffer) and the init segment, then pause. Do not add the client’s onData to the streaming list until a keyframe moof is available. Essentially, the new client will sit idle (up to ~idrInterval seconds) until the next IDR arrives from the camera. At that moment, send the IDR fragment (moof+mdat) as the first video data to that client, then continue streaming subsequent fragments. This guarantees the first appended frame is a full keyframe, allowing the decoder to start outputting video. A short startup delay (e.g. up to 1 second) is acceptable because it ensures stable playback rather than a stall.
Send the Last Keyframe (Cached): For an even faster start, you can cache the most recent keyframe fragment from the existing stream and send it immediately to new clients. For example: when client #2 connects, if the stream’s last IDR frame occurred 0.5 seconds ago, send that cached keyframe plus any stored subsequent moof chunks up to real-time. This effectively “splices in” the new client slightly behind live, but they will quickly catch up as playback proceeds. The crucial part is that their very first appended segment contains an IDR. This approach avoids waiting for the next keyframe, at the cost of slightly more complexity (you need to buffer and manage a small segment of video). Many streaming servers do something similar to let new subscribers start on a keyframe without noticeable delay.
Adjust Playback Position (if needed): If the buffered range for the new client doesn’t start at 0, we can programmatically seek the video element to the start of the buffered range. For instance, after appending the first keyframe segment, do: video.currentTime = sourceBuffer.buffered.start(0). This ensures the video’s playhead jumps to the time of the first frame. In most cases, if we start with a keyframe and append continuously, the player will begin automatically. But explicitly setting the currentTime can help skip any initial gap if the timeline isn’t zero-based. (This is more of a client-side tweak – the main fix is still to supply a keyframe.)
Conclusion
Why the thumbnail works but the modal doesn’t: The thumbnail (first client) began at the start of a new stream, so it got an init segment followed by a keyframe (IDR) and then other frames – decoding started normally. The modal (second client) was attached to an ongoing stream and was fed an init segment followed by non-key frames, which the decoder discarded or could not display. The lack of an initial keyframe is the root cause of the black screen in the modal view
. This is a known requirement in video streaming: a decoder needs an IDR frame to begin; joining in the middle of inter-predicted frames will stall. By modifying the server logic to introduce new clients at keyframe boundaries, we resolve the issue. In practice, that means detecting or caching IDR frames and sending the init segment + first keyframe fragment to the new client (with a slight delay or from cache). With this change, the second client’s video should initialize correctly — it will display the video a fraction of a second after connecting, once it has the keyframe and a few subsequent fragments to play. The buffered range will then show a proper span (e.g. 0.00–X.XXs from the client’s perspective), and the video will play smoothly from that point onward.



Quellen



Keine Datei ausgewähltKeine Datei ausgewählt
ChatGPT kann Fehler machen. OpenAI verwendet keine Daten aus dem Arbeitsbereich Hengl zum Trainieren seiner Modelle.

MSE Livestream Playback Start from Keyframe in fMP4
ChatGPT
Ich schaue mir die detaillierten Richtlinien und die potentiellen Ursachen des Problems genauer an.

Searched for WebRTCPlayer in HASS-Dashboard repo
ChatGPT
Die Suche zeigt Ergebnisse an, darunter "packages/web/src/components/WebRTCPlayer.tsx", was auf eine mögliche Coding-Komponente hindeuten könnte.


GitHub gelesen

ChatGPT
Ich schaue mir das Problem an, dass der zweite Client beim Streamen keine Bilder zeigt. Gut möglich, dass es an fehlenden Keyframes liegt.

Gesucht nach Media Source Extensions Keyframe Black Screen
ChatGPT
Es ergibt sich hier ein Zusammenhang: Ein Video bleibt bei Verwendung der Media Source Extension API hängen und zeigt auch bei voller Länge lediglich einen schwarzen Bildschirm.


stackoverflow.com gelesen

ChatGPT
Es wird darauf hingewiesen, dass der schwarze Bildschirm durch einen Start bei 1 Sekunde verursacht wird, gefolgt von einem Blick auf mögliche Lösungen auf Stack Overflow.


Mehr von stackoverflow.com gelesen

ChatGPT
Lernen Sie die Optionen zu manuell durchsuchen.


Mehr von stackoverflow.com gelesen

ChatGPT
Ich schaue nach detaillierten Anweisungen zu Keyframes und deren Erfordernissen für MSE, insbesondere beim Mitmachen eines Livestreams.

Gesucht nach MediaSource Live Stream Keyframe Moof
ChatGPT
Dies scheint ein Feature- oder Fehlerproblem im Zusammenhang mit dem Erfordernis von Keyframes für Mediensegmente in Chromium zu sein.


issues.chromium.org gelesen

ChatGPT
Ich schaue mir den Blogpost "Streaming Media on Demand with MSE" an, um die Schlüsselbildanforderung bei mittenem Start zu klären.


hacks.mozilla.org gelesen

ChatGPT
Ich erforsche das detaillierte Verfahren und dessen Wirkung.

Gesucht nach MediaSource SourceBuffer Keyframe erforderlich
ChatGPT
Es scheint, dass der Mediasource Sourcebuffer Fehler beim Laden älterer Blobs auftritt und möglicherweise mit dem Video Playback auf einem Keyframe zusammenhängt.


stackoverflow.com gelesen

ChatGPT
Plattform-Informationsaustausch auf keyframe überprüfen. Sekundärer Client benötigt keyframe, um Wiedergabe zu starten.


Mehr von stackoverflow.com gelesen

ChatGPT
Es scheint, dass das zweite Klient im H.264-Stream kein IDR-Frame erhält, was zu einem schwarzen Bildschirm führt. Eine Nahtstelle zwischen der Topologie und der Ipsec Verschlüsselung könnte hier ebenfalls eine Rolle spielen.

Gesucht nach "sample_is_non_sync_sample" moof trun keyframe
ChatGPT
Das Problem ist, dass neue Clients in der Regel mit einem Keyframe beginnen sollten. Dies könnte durch das Warten auf den nächsten Keyframe oder durch Vorratsstellung des letzten Keyframes erreicht werden.

Gesucht nach unifi-protect idrInterval Keyframe fMP4
ChatGPT
Ich schaue mir das idrInterval in Unifi Protect an, das häufig auf 1 Sekunde gesetzt ist, um Ladezeiten zu verringern.


docs.scrypted.app gelesen

ChatGPT
Im Hinblick auf den Codec forsche ich, ob der H.264-Codec in beiden Fällen identisch ist und keine Unterschiede verursacht.

Gesucht nach "idrInterval" protect-livestream

b.goeswhere.com gelesen

ChatGPT
Ich schaue mir die Detailseiten genau an, um den Begriff "nicht synchron" zu finden.


Mehr von b.goeswhere.com gelesen

ChatGPT
Ich denke darüber nach, wie die Worte „OPEN“ und „unghi“ aus einer weiteren Perspektive ähnlich klingen könnten.


Mehr von b.goeswhere.com gelesen

ChatGPT
Ich schaue mir das Timing von Keyframes und GOP an. Wenn ein Client mitten im Stream beitrit, hilft es, bis zum nächsten Keyframe zu warten, um stabile Wiedergabe zu gewährleisten.

Ich schaue mir an, wie sich Beitrittszeiten auf die Wiedergabe auswirken. Um Stabilität zu gewährleisten, schlage ich vor, bis zum nächsten Keyframe zu warten oder den vorherigen Keyframe fragmentarisch vorzuspielen.

Gesucht nach MediaSource Live Stream Buffered Range
ChatGPT
Ich schaue mir Stackoverflow Ergebnis 2 an, das zeigt, wie man ein Video zu einem bestimmten Zeitpunkt startet.


stackoverflow.com gelesen

ChatGPT
Ich denke darüber nach, wie clientspezifische Zeilenanpassungen die Synchronität optimieren könnten. Kombinationen wie timestampOffset und Quellenpuffer sind spannend. Ich werde die Auswirkungen auf die Wiedergabezeit und Effizienz ausloten.

Gesucht nach "avc1.640020" "unifi protect"
ChatGPT
Es scheint vertrauenswürdig zu sein, dass Protect h264 verwendet, basierend auf bekannten Kontexten oder möglicherweise aus Protokollen oder API-Herausgaben entnommen.


reddit.com gelesen


github.com gelesen

ChatGPT
Ich schaue mir die MSE-Spezifikation an und merke, dass nicht alle Segmente mit einem Schlüsselbild starten müssen, was kontinuierliches Streaming erleichtert.


issues.chromium.org gelesen

ChatGPT
Ich schaue mir die Spezifikationen [14L378-L386] und [21L5251-L5258**](powered by OpenAI) an, um zu klären, wann Keyframes in moof-Streams erforderlich sind und was die sample_is_non_sync_sample=0 Flag bedeutet.


community.ui.com gelesen

ChatGPT
Ich schaue mir die Anwendungsfälle an und sehe, dass zum Beispiel Protect eine 5-Sekunden-Schlüsselbildintervall-Einstellung verwendet, die auf 1 Sekunde geändert werden kann, um Streams flüssiger zu gestalten Anwenden der Optionen.

Gesucht nach unifi protect livestream in HASS-Dashboard repo
ChatGPT
diese Route tamen

Research completed in 14m· 17 Quellen

MSE Livestream Black Screen – Root Cause and Solution
Codec and Initialization Segment
UniFi Protect cameras stream H.264 video (commonly High or Main Profile, e.g. avc1.640020 for high-res feeds). The codec and init segment (ftyp+moov) are identical for all clients, and the first client’s successful playback confirms these are correct. In other words, there is no codec mismatch or corruption causing the second client’s black screen. Reusing the same codec string and initialization segment for new viewers is fine – the issue lies elsewhere (likely in timing and video frame dependencies, not in the codec data itself).
Keyframe Interval (GOP Length) in Protect Streams
UniFi Protect cameras typically produce an I-frame (keyframe) about every 1 second by default (configurable per camera). This keyframe interval (often called IDR interval or GOP length) means all intermediate frames rely on the last keyframe. For example, if the camera is 20 FPS with a 1-second GOP, you get one IDR frame followed by ~19 P-frames until the next keyframe. (In some versions, Protect’s default was a 5-second interval, but it’s usually set to 1s for low-latency streaming.) The unifi-protect API even exposes an idrInterval setting for each video channel, indicating how many frames (or seconds) between keyframes. The keyframe frequency is crucial for new clients: it determines the maximum delay before a joining client can see a decodable frame.
Joining Mid-Stream Requires a Keyframe
The core issue is that the second (modal) client is joining the stream in the middle of a GOP, without an initial keyframe. In H.264, video playback must start on a keyframe – if you begin feeding the decoder only interdependent P-frames/B-frames with no preceding IDR frame, the decoder has no reference and cannot render a picture
. This is why the thumbnail (first client) works: it started from the stream’s beginning (which began with an IDR in the init segment), so every frame had the needed reference. The modal client, however, attached to an ongoing session and likely received a “moof” fragment containing only delta frames at first. Without the corresponding IDR, those frames “make no sense and produce no picture on their own”
. As a result, the video element remains black until a keyframe arrives. Additional Timing Factor: When the second client appends its first data chunk, that fragment’s timestamps are likely well past 0 (e.g. 10s into the stream). The media buffer now starts at a later time with nothing buffered at t=0. The video element by default begins at currentTime = 0, outside the buffered range – leading to a stall. The player will not display anything until playback position reaches the buffered range. In practice, the second client’s video tag is stuck at time 0 with no frame to show. Even though the code calls video.play(), the player cannot start rendering because it hasn’t received a keyframe at the current time. This compound effect (missing IDR + timeline gap) explains why the modal view shows no video and why the buffered TimeRange appeared “none or very small.” It’s not that the cached init segment is wrong – it’s that the first media fragment for the second client was not a sync frame, and thus nothing decoded or added to the playable buffer.
Verifying Keyframes in fMP4 Stream
Yes – we can detect keyframe boundaries on the server side. In fragmented MP4 (fMP4), each moof box contains track run entries (trun) with flags indicating frame type. Specifically, there’s a sample_flags field where the sample_is_non_sync_sample bit is set for non-key frames. If that flag is 0 for the first sample in a fragment, it means the fragment starts with a sync sample (keyframe)
. In other words, by parsing each incoming moof from the Protect stream, the server can identify whether it begins with a keyframe. The UniFi Protect library doesn’t explicitly call this out for us, but we can parse the bytes or use a library (e.g. mp4box or similar) to check those flags. Additionally, since we know the camera’s keyframe interval (e.g. ~1 second), we expect roughly every Nth fragment to be an IDR. Many streaming implementations fragment at keyframe boundaries by design (ftyp+moov init, then each moof+mdat contains one GOP or starts with an IDR). We should confirm if Protect’s fMP4 behaves this way. If so, each IDR frame will coincide with a new fragment, making detection easier. Keyframe caching: It would be wise to store the last keyframe fragment in memory. For example, whenever a moof with a sync sample comes through, keep a copy of that moof+mdat. This way, when a new client joins, we have an IDR frame ready to send (even if the next keyframe is nearly a full interval away). We could either send this cached keyframe immediately or use it as a fallback if the next one is delayed.
Solution – Start New Clients at a Keyframe
To fix the black-screen issue, ensure that any new viewer begins receiving data at a keyframe boundary. There are a couple of approaches to achieve this:
Wait for the Next Keyframe: When a second client connects to an existing session, do not immediately pipe through the live P-frames. Instead, send them the codec info (so they can create a SourceBuffer) and the init segment, then pause. Do not add the client’s onData to the streaming list until a keyframe moof is available. Essentially, the new client will sit idle (up to ~idrInterval seconds) until the next IDR arrives from the camera. At that moment, send the IDR fragment (moof+mdat) as the first video data to that client, then continue streaming subsequent fragments. This guarantees the first appended frame is a full keyframe, allowing the decoder to start outputting video. A short startup delay (e.g. up to 1 second) is acceptable because it ensures stable playback rather than a stall.
Send the Last Keyframe (Cached): For an even faster start, you can cache the most recent keyframe fragment from the existing stream and send it immediately to new clients. For example: when client #2 connects, if the stream’s last IDR frame occurred 0.5 seconds ago, send that cached keyframe plus any stored subsequent moof chunks up to real-time. This effectively “splices in” the new client slightly behind live, but they will quickly catch up as playback proceeds. The crucial part is that their very first appended segment contains an IDR. This approach avoids waiting for the next keyframe, at the cost of slightly more complexity (you need to buffer and manage a small segment of video). Many streaming servers do something similar to let new subscribers start on a keyframe without noticeable delay.
Adjust Playback Position (if needed): If the buffered range for the new client doesn’t start at 0, we can programmatically seek the video element to the start of the buffered range. For instance, after appending the first keyframe segment, do: video.currentTime = sourceBuffer.buffered.start(0). This ensures the video’s playhead jumps to the time of the first frame. In most cases, if we start with a keyframe and append continuously, the player will begin automatically. But explicitly setting the currentTime can help skip any initial gap if the timeline isn’t zero-based. (This is more of a client-side tweak – the main fix is still to supply a keyframe.)
Conclusion
Why the thumbnail works but the modal doesn’t: The thumbnail (first client) began at the start of a new stream, so it got an init segment followed by a keyframe (IDR) and then other frames – decoding started normally. The modal (second client) was attached to an ongoing stream and was fed an init segment followed by non-key frames, which the decoder discarded or could not display. The lack of an initial keyframe is the root cause of the black screen in the modal view
. This is a known requirement in video streaming: a decoder needs an IDR frame to begin; joining in the middle of inter-predicted frames will stall. By modifying the server logic to introduce new clients at keyframe boundaries, we resolve the issue. In practice, that means detecting or caching IDR frames and sending the init segment + first keyframe fragment to the new client (with a slight delay or from cache). With this change, the second client’s video should initialize correctly — it will display the video a fraction of a second after connecting, once it has the keyframe and a few subsequent fragments to play. The buffered range will then show a proper span (e.g. 0.00–X.XXs from the client’s perspective), and the video will play smoothly from that point onward.
Quellenangaben

javascript - Mediasource sourcebuffer error when loading older blobs from a Mediarecorder - Stack Overflow

https://stackoverflow.com/questions/79043134/mediasource-sourcebuffer-error-when-loading-older-blobs-from-a-mediarecorder
Microsoft Word - C068960e.doc

https://b.goeswhere.com/ISO_IEC_14496-12_2015.pdf
Alle Quellen

stackoverflow
b.goeswhere